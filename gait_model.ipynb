{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94feb64d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from math import inf\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define a pytorch Dataset object to contain the training and testing data\n",
    "\n",
    "Pytorch handles data shuffling and batch loading, as long as the user provides a \"Dataset\" class. This class is just a\n",
    "wrapper for your data that casts the data into pytorch tensor format and returns slices of the data. In this case, our\n",
    "data is in numpy format, which conveniently pytorch has a method for converting to their native format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AngleDataset(Dataset):\n",
    "    def __init__(self, csv_path: Path):\n",
    "        \"\"\"Create a PyTorch dataset from CSV data.\n",
    "\n",
    "        Args:\n",
    "            csv_path (Path): kinematics data file\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        length = len(df)\n",
    "\n",
    "        # The network takes a sinusoidal signal as an input\n",
    "        sim_time = 10\n",
    "        freq = 1\n",
    "        time = torch.linspace(0, sim_time, length)\n",
    "        sin_signal = torch.sin(2 * torch.pi * freq * time)\n",
    "\n",
    "        # data order = sin (1), angles (4*3*2), torso (2*2), touch_sens (4)\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "\n",
    "        for sin_val, i in zip(sin_signal, range(length)):\n",
    "            x_data.append(torch.hstack([sin_val, torch.tensor(df.iloc[i])]))\n",
    "            # No need to predict/output touch sensor values\n",
    "            y_data.append(df.iloc[i + 1][:-4] if i < length - 1 else df.iloc[0][:-4])\n",
    "\n",
    "        self.x_data: torch.Tensor = torch.vstack(x_data).type(torch.float32)\n",
    "        self.y_data: torch.Tensor = torch.tensor(y_data).type(torch.float32)\n",
    "        self.length: int = len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define training methods for the model\n",
    "\n",
    "These methods use an initialized model and training data to iteratively perform the forward and backward pass of\n",
    "optimization. Aside from some data reformatting that depends on the input, output, and loss function, these methods will\n",
    "always be the same for any shallow neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_batch(\n",
    "    model: nn.Module,\n",
    "    x: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criteria: nn.Module,\n",
    ") -> float:\n",
    "    \"\"\"Train the given model on a single batch of data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): model to train\n",
    "        x (torch.Tensor): input data\n",
    "        y (torch.Tensor): labeled output data\n",
    "        optimizer (optim.Optimizer): SGD-based optimizer\n",
    "        criteria (nn.Module): loss function\n",
    "\n",
    "    Returns:\n",
    "        float: mean loss for the batch\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Compute output\n",
    "    y_predict = model(x)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criteria(y_predict, y)\n",
    "\n",
    "    # Zero out current gradient values\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data.item()\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    num_epochs: int,\n",
    ") -> list[float]:\n",
    "    \"\"\"Train the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): model to train\n",
    "        loader (DataLoader): training data\n",
    "        optimizer (nn.Module): SGD-based optimizer\n",
    "        criterion (nn.Module): loss function\n",
    "        num_epochs (int): number of epochs to train\n",
    "\n",
    "    Returns:\n",
    "        list[float]: mean loss for each batch in each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for x, y in loader:\n",
    "            loss = train_batch(model, x, y, optimizer, criterion)\n",
    "            losses.append(loss)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define inference methods for the model\n",
    "\n",
    "These methods are like training, but we don't need to update the parameters of the model anymore because when we call\n",
    "the test() method, the model has already been trained. Instead, this method just calculates the predicted y values and\n",
    "returns them, AKA the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_inference(model: nn.Module, x: torch.Tensor) -> torch.Tensor:\n",
    "    model.eval()\n",
    "    return model(x)\n",
    "\n",
    "\n",
    "def inference(model: nn.Module, dataset: AngleDataset) -> torch.Tensor:\n",
    "    \"\"\"Compute model outputs for the given data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): trained model\n",
    "        dataset (AngleDataset): data to test\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: computed output values\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=len(dataset))\n",
    "    predictions = [batch_inference(model, x) for x, _ in loader]\n",
    "    return torch.concat(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define Model Architecture\n",
    "- 33 inputs = 3 joint angles per leg, 4 legs, 2 DOF per joint. 4 touch sensors. 1 sine timestamp.\n",
    "- 28 outputs = *same as above, except just the joint angles*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GaitModel(nn.Module):\n",
    "    def __init__(self, layer_sizes: list[int], batch_norm: bool, dropout: float):\n",
    "        \"\"\"A PyTorch model trained to output new joint angles.\n",
    "\n",
    "        Args:\n",
    "            layer_sizes (list[int]): number of neurons per layer\n",
    "            batch_norm (bool): flag for using batch normalization\n",
    "            dropout (float): flag/value for using dropout\n",
    "        \"\"\"\n",
    "        super(GaitModel, self).__init__()\n",
    "\n",
    "        hidden_layers = []\n",
    "\n",
    "        # Loop over layer_sizes and create Linear->ReLU[->BatchNorm][->Dropout] layers\n",
    "        for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes):\n",
    "            # Required layers\n",
    "            layers = [nn.Linear(nlminus1, nl), nn.ReLU()]\n",
    "\n",
    "            # Optional batch normalization layer\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(nl))\n",
    "\n",
    "            # Optional dropout layer\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "\n",
    "            hidden_layers.append(nn.Sequential(*layers))\n",
    "\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define Run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset: AngleDataset,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    layer_sizes: list[int],\n",
    "    batch_norm: bool,\n",
    "    dropout: float,\n",
    ") -> tuple[nn.Module, list[float]]:\n",
    "    \"\"\"Train a model on the given dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (AngleDataset): kinematics dataset\n",
    "        layer_sizes (list[int]): neurons per layer\n",
    "        batch_norm (bool): batch normalization flag\n",
    "        dropout (float): dropout value (0 for no dropout)\n",
    "        num_epochs (int): number of training epochs\n",
    "        batch_size (int): training batch size\n",
    "        learning_rate (float): training learning rate\n",
    "\n",
    "    Returns:\n",
    "        tuple[nn.Module, list[float]]: model and training losses\n",
    "    \"\"\"\n",
    "\n",
    "    # learning_rate = 1e-3\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    model = GaitModel(layer_sizes, batch_norm, dropout)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    losses = train_loop(\n",
    "        model=model,\n",
    "        loader=data_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = Path(\"KinematicsData/\")\n",
    "FIGURE_DIR = Path(\"Figures/\")\n",
    "MODEL_OUTPUT_DIR = Path(\"ModelOutputs/\")\n",
    "MODEL_DIR = Path(\"Models/\")\n",
    "\n",
    "df = pd.read_csv(next(DATA_DIR.glob(\"*_kinematic.csv\")))\n",
    "\n",
    "# We only need the first 28 columns (ignore the touch sensors for output)\n",
    "CSV_HEADER = df.columns[:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "datasets = {\n",
    "    f.stem.split(\"_\")[0]: AngleDataset(f) for f in DATA_DIR.glob(\"*_kinematic.csv\")\n",
    "}\n",
    "\n",
    "# Model hyperparameters\n",
    "layer_sizes = [33, 31, 30, 28]\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "for gait_name in datasets:\n",
    "\n",
    "    dataset = datasets[gait_name]\n",
    "\n",
    "    print(f\"Processing the '{gait_name}' dataset.\")\n",
    "\n",
    "    model, losses = train(\n",
    "        dataset,\n",
    "        num_epochs,\n",
    "        batch_size,\n",
    "        learning_rate,\n",
    "        layer_sizes,\n",
    "        batch_norm=False,\n",
    "        dropout=0,\n",
    "    )\n",
    "\n",
    "    predictions = inference(model, dataset)\n",
    "\n",
    "    # Save the outputs to a csv for quicker comparisons later\n",
    "    with open(MODEL_OUTPUT_DIR / f\"{gait_name}_output.csv\", \"w\") as csv_file:\n",
    "\n",
    "        writer = csv.writer(csv_file)\n",
    "\n",
    "        writer.writerow(CSV_HEADER)\n",
    "\n",
    "        for row in predictions:\n",
    "            writer.writerow(row.tolist())\n",
    "\n",
    "    torch.save(model, MODEL_DIR / f\"{gait_name}_model.pt\")\n",
    "\n",
    "    final_loss = sum(losses[-100:]) / 100\n",
    "\n",
    "    print(f\"Final loss for {gait_name}: {final_loss}\")\n",
    "\n",
    "    plt.plot(losses, label=gait_name)\n",
    "\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(FIGURE_DIR / f\"losses.png\", facecolor=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f6972",
   "metadata": {},
   "source": [
    "## Sanity Check Data By Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03291bc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "kinematics = sorted([f for f in DATA_DIR.glob(\"*_kinematic.csv\") if f.is_file()])\n",
    "outputs = sorted([f for f in MODEL_OUTPUT_DIR.glob(\"*_output.csv\") if f.is_file()])\n",
    "\n",
    "for actual, pred in zip(kinematics, outputs):\n",
    "\n",
    "    dfa = pd.read_csv(actual)\n",
    "    dfp = pd.read_csv(pred)\n",
    "\n",
    "    columns = dfp.columns\n",
    "    num_cols = len(columns)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        num_cols // 2,\n",
    "        2,\n",
    "        figsize=(16, 64),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "\n",
    "    gait_name = actual.stem.split(\"_\")[0]\n",
    "\n",
    "    fig.suptitle(gait_name, fontsize=24)\n",
    "\n",
    "    for col, ax in zip(columns, axes.flatten()):\n",
    "        ax.plot(dfa[col], label=\"Actual\")\n",
    "        ax.plot(dfp[col], label=\"Prediction\")\n",
    "        ax.set_title(col)\n",
    "        ax.legend()\n",
    "\n",
    "    fig.savefig(FIGURE_DIR / f\"{gait_name}_comparison.png\", facecolor=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupytext --set-formats ipynb, py:percent gait_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3017cd4797bf12a2c81ef5ba3b65ed0d92f944fa0709225dc40687ba16375871"
  },
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
