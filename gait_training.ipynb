{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94feb64d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from math import radians\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "225d14e6",
   "metadata": {},
   "source": [
    "# Define a pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d493970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinematicsNoiseTransform(object):\n",
    "    \"\"\"Add gaussian noise to a tensor.\"\"\"\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "        # TODO: hardcoded for now\n",
    "        self.num_gauss = 24\n",
    "        self.num_touch = 4\n",
    "        self.touch_index = self.num_gauss\n",
    "        \n",
    "    def __call__(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        new_tensor = tensor.clone()\n",
    "\n",
    "        # Add noise to angles\n",
    "        new_tensor[:self.num_gauss] += torch.randn_like(tensor[:self.num_gauss]) * self.std + self.mean\n",
    "\n",
    "        # Randomly flip touch sensors (and don't do anything with the sinusoid)\n",
    "        new_tensor[self.touch_index:-1] = torch.randint_like(tensor[self.touch_index:-1], 0, 2)\n",
    "        # new_tensor[self.touch_index:-1] = 0\n",
    "\n",
    "        # Don't add noise to sinusoid\n",
    "        return new_tensor\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d9093",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinematicsDataset(Dataset):\n",
    "    def __init__(self, csv_path: Path, transform=None):\n",
    "        \"\"\"Create a PyTorch dataset from CSV data.\n",
    "\n",
    "        Args:\n",
    "            csv_path (Path): kinematics data file\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.columns = df.columns\n",
    "\n",
    "        # NOTE: max_radians_scale must match simulation\n",
    "        max_radians_scale = radians(120)\n",
    "        df.iloc[:,:24] = df.iloc[:,:24].div(max_radians_scale, axis=0)\n",
    "\n",
    "        # NOTE: frequency and time_step must match simulation\n",
    "        frequency = 1\n",
    "        time_step = 0.02\n",
    "        df[\"Sine\"] = np.sin(2 * np.pi * frequency * np.arange(0, len(df) * time_step, time_step))\n",
    "\n",
    "        # Data order (29 columns: 24 joints, 4 touch, 1 sine):\n",
    "        # - Front left hip dof1\n",
    "        # - Front left hip dof2\n",
    "        # - Front right hip dof1\n",
    "        # - ...\n",
    "        # - Front left touch sensor\n",
    "        # - Front right touch sensor\n",
    "        # - Rear left touch sensor\n",
    "        # - Rear right touch sensor\n",
    "        # - Sinusoid\n",
    "\n",
    "        # Input includes all but the final row\n",
    "        self.X = torch.tensor(df.values[:-1, :], dtype=torch.float32)\n",
    "\n",
    "        # Output includes all but the first row and the touch sensor columns\n",
    "        self.Y = torch.tensor(df.values[1:, :24], dtype=torch.float32)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.X[index]\n",
    "        return self.transform(x) if self.transform else x, self.Y[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7cc7061",
   "metadata": {},
   "source": [
    "# Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d453ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaitModel(nn.Module):\n",
    "    def __init__(self, layer_sizes: list[int], batch_norm: bool, dropout: float):\n",
    "        \"\"\"A PyTorch model trained to output new joint angles.\n",
    "\n",
    "        Args:\n",
    "            layer_sizes (list[int]): number of neurons per layer\n",
    "            batch_norm (bool): flag for using batch normalization\n",
    "            dropout (float): flag/value for using dropout\n",
    "        \"\"\"\n",
    "        super(GaitModel, self).__init__()\n",
    "\n",
    "        hidden_layers = []\n",
    "\n",
    "        # Loop over layer_sizes and create linear->relu[->batchnorm][->dropout] layers\n",
    "        for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes):\n",
    "            # Required layers\n",
    "            layers = [nn.Linear(nlminus1, nl), nn.ReLU()]\n",
    "\n",
    "            # Optional batch normalization layer\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(nl))\n",
    "\n",
    "            # Optional dropout layer\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "\n",
    "            hidden_layers.append(nn.Sequential(*layers))\n",
    "\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "        # Print a summary of the model\n",
    "        summary(self)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14042018",
   "metadata": {},
   "source": [
    "# Define training methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(\n",
    "    model: nn.Module,\n",
    "    X: torch.Tensor,\n",
    "    Y: torch.Tensor,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criteria: nn.Module,\n",
    ") -> float:\n",
    "    \"\"\"Train the given model on a single batch of data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): model to train\n",
    "        X (torch.Tensor): input data\n",
    "        Y (torch.Tensor): labeled output data\n",
    "        optimizer (optim.Optimizer): SGD-based optimzier\n",
    "        criteria (nn.Module): loss function\n",
    "\n",
    "    Returns:\n",
    "        float: mean loss for the batch\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Compute output\n",
    "    Y_predict = model(X)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criteria(Y_predict, Y)\n",
    "\n",
    "    # Zero out current gradient values\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data.item()\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    critera: nn.Module,\n",
    "    num_epochs: int,\n",
    ") -> list[float]:\n",
    "    \"\"\"Train the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): model to train\n",
    "        loader (DataLoader): training data\n",
    "        optimizer (nn.Module): SGD-based optimizer\n",
    "        critera (nn.Module): loss function\n",
    "        num_epochs (int): number of epochs to train\n",
    "\n",
    "    Returns:\n",
    "        list[float]: mean loss for each batch in each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for X, Y in loader:\n",
    "            loss = train_batch(model, X, Y, optimizer, critera)\n",
    "            losses.append(loss)\n",
    "\n",
    "    return losses\n",
    "\n",
    "def train(\n",
    "    dataset: KinematicsDataset,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    layer_sizes: list[int],\n",
    "    batch_norm: bool,\n",
    "    dropout: float,\n",
    ") -> tuple[nn.Module, list[float]]:\n",
    "    \"\"\"Train a model on the given dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (AngleDataset): kinematics dataset\n",
    "        layer_sizes (list[int]): neurons per layer\n",
    "        batch_norm (bool): batch normalization flag\n",
    "        dropout (float): dropout value (0 for no dropout)\n",
    "        num_epochs (int): number of training epochs\n",
    "        batch_size (int): training batch size\n",
    "        learning_rate (float): training learning rate\n",
    "\n",
    "    Returns:\n",
    "        tuple[nn.Module, list[float]]: model and training losses\n",
    "    \"\"\"\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    model = GaitModel(layer_sizes, batch_norm, dropout)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    losses = train_loop(\n",
    "        model=model,\n",
    "        loader=data_loader,\n",
    "        optimizer=optimizer,\n",
    "        critera=criterion,\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    return model, losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc6952e4",
   "metadata": {},
   "source": [
    "# Define inference methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ddef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference(model: nn.Module, X: torch.Tensor) -> torch.Tensor:\n",
    "    model.eval()\n",
    "    return model(X)\n",
    "\n",
    "\n",
    "def inference(model: nn.Module, dataset: KinematicsDataset) -> torch.Tensor:\n",
    "    \"\"\"Compute model outputs for the given data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): trained model\n",
    "        dataset (AngleDataset): data to test\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: computed output values\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=len(dataset))\n",
    "    predictions = [batch_inference(model, x) for x, _ in loader]\n",
    "    return torch.concat(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c2074d6",
   "metadata": {},
   "source": [
    "# Train models for each gait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"MotionData/\")\n",
    "FIGURE_DIR = Path(\"Figures/\")\n",
    "MODEL_DIR = Path(\"Models/\")\n",
    "\n",
    "# Add noise to dataset inputs\n",
    "transform = KinematicsNoiseTransform(0, 0.1)\n",
    "\n",
    "# Load all datasets and take a peek at the first\n",
    "datasets = { f.stem.split(\"_\")[0]: KinematicsDataset(f, transform=transform) for f in DATA_DIR.glob(\"*_kinematic.csv\") }\n",
    "csv_header = list(datasets.values())[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "num_input = 29 # 24 angles, 4 touch sensors, 1 sinusoid\n",
    "num_output = 24 # 24 angles for next time step\n",
    "layer_sizes = [num_input, 32, 32, num_output]\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 25\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "for gait_name in datasets:\n",
    "\n",
    "    dataset = datasets[gait_name]\n",
    "\n",
    "    print(f\"Processing the '{gait_name}' dataset.\")\n",
    "\n",
    "    model, losses = train(\n",
    "        dataset,\n",
    "        num_epochs,\n",
    "        batch_size,\n",
    "        learning_rate,\n",
    "        layer_sizes,\n",
    "        batch_norm=False,\n",
    "        dropout=0,\n",
    "    )\n",
    "\n",
    "    predictions = inference(model, dataset)\n",
    "\n",
    "    # Save the outputs to a csv for quicker comparisons later\n",
    "    with open(DATA_DIR / f\"{gait_name}_model.csv\", \"w\") as csvfile:\n",
    "\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        writer.writerow(csv_header)\n",
    "\n",
    "        for row in predictions:\n",
    "            writer.writerow(row.tolist())\n",
    "\n",
    "    torch.save(model, MODEL_DIR / f\"{gait_name}_model.pt\")\n",
    "\n",
    "    final_loss = sum(losses[-100:]) / 100\n",
    "\n",
    "    print(f\"Final loss for {gait_name}: {final_loss}\")\n",
    "\n",
    "    plt.plot(losses, label=gait_name)\n",
    "\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(FIGURE_DIR / f\"losses.png\", facecolor=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff9c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp Models/walk_model.pt ../Python_ODE_3/Experiments/Bio_Gaits_Pretraining/pretrained_brains/walk_model.pt\n",
    "!cp MotionData/walk_kinematic.csv ../Python_ODE_3/Experiments/Bio_Gaits_Pretraining/kinematic/walk_kinematic.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f6972",
   "metadata": {},
   "source": [
    "## Sanity Check Data By Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03291bc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "kinematics = sorted([f for f in DATA_DIR.glob(\"*_kinematic.csv\") if f.is_file()])\n",
    "outputs = sorted([f for f in DATA_DIR.glob(\"*_model.csv\") if f.is_file()])\n",
    "\n",
    "for actual, pred in zip(kinematics, outputs):\n",
    "\n",
    "    # Radians should match simulation\n",
    "    dfa = pd.read_csv(actual) / radians(120)\n",
    "    dfp = pd.read_csv(pred)\n",
    "\n",
    "    columns = dfp.columns\n",
    "    num_cols = len(columns)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        num_cols // 2,\n",
    "        2,\n",
    "        figsize=(16, 64),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "\n",
    "    gait_name = actual.stem.split(\"_\")[0]\n",
    "\n",
    "    fig.suptitle(gait_name, fontsize=24)\n",
    "\n",
    "    for col, ax in zip(columns, axes.flatten()):\n",
    "        ax.plot(dfa[col], label=\"Actual\")\n",
    "        ax.plot(dfp[col], label=\"Prediction\")\n",
    "        ax.set_title(col)\n",
    "        ax.legend()\n",
    "\n",
    "    fig.savefig(FIGURE_DIR / f\"{gait_name}_comparison.png\", facecolor=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5180f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupytext --sync gait_training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c095f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "8f9c6a430889c2bc7b312b3a36b1972d2b7c7bf948955b41b3f00e7c6f0b0380"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
